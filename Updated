import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
import nibabel as nib
from PIL import Image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import f1_score
from torch.utils.tensorboard import SummaryWriter
import torch.nn.functional as F

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Path to the ISLES 2022 dataset
data_dir = "/media/bioeseniordesign/internal/islesdata/dataset_dir/ISLES-2022"
# Create a directory to store TensorBoard logs
log_dir = "/media/bioeseniordesign/internal/islesdata/dataset_dir/logs"

if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# Initialize a SummaryWriter to write TensorBoard logs
writer = SummaryWriter(log_dir=log_dir)

def load_isles_data(data_dir):
    subjects = []
    for i in range(1, 251):
        id_ = f"sub-strokecase{i:04d}"
        print(f"id: {id_} done.")

        dwi_path = os.path.join(data_dir, f"rawdata/{id_}/ses-0001/dwi/{id_}_ses-0001_dwi.nii.gz")
        flair_path = os.path.join(data_dir, f"rawdata/{id_}/ses-0001/anat/{id_}_ses-0001_flair_registered.nii.gz")
        adc_path = os.path.join(data_dir, f"rawdata/{id_}/ses-0001/dwi/{id_}_ses-0001_adc.nii.gz")
        mask_path = os.path.join(data_dir, f"derivatives/{id_}/ses-0001/{id_}_ses-0001_msk.nii.gz")

        if os.path.exists(dwi_path) and os.path.exists(flair_path) and os.path.exists(adc_path) and os.path.exists(mask_path):
            dwi = nib.load(dwi_path).get_fdata()
            flair = nib.load(flair_path).get_fdata()
            adc = nib.load(adc_path).get_fdata()
            mask = nib.load(mask_path).get_fdata()
            subjects.append({'id': id_, 'dwi': dwi, 'flair': flair, 'adc': adc, 'mask': mask})
        else:
            print(f"Data for {id_} is incomplete. Skipping...")
    return subjects

# Define transformations for the image and mask separately
image_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

mask_transform = transforms.Compose([
    transforms.ToTensor(),
])

class ISLESSegmentationDataset(Dataset):
    def __init__(self, subjects, image_transform=None, mask_transform=None):
        self.subjects = subjects
        self.image_transform = image_transform
        self.mask_transform = mask_transform

    def __len__(self):
        return len(self.subjects)

    def preprocess_image(self, img, slice_idx, output_size=(256, 256)):
        img_slice = img[..., slice_idx]
        if np.max(img_slice) == np.min(img_slice):  # Handle division by zero
            img_normalized = np.zeros(img_slice.shape)
        else:
            img_normalized = (img_slice - np.min(img_slice)) / (np.max(img_slice) - np.min(img_slice))
        img_rescaled = (img_normalized * 255).astype(np.uint8)
        img_pil = Image.fromarray(img_rescaled)
        img_resized = img_pil.resize(output_size)
        return img_resized

    def __getitem__(self, idx):
        subject = self.subjects[idx]
        id_ = subject['id']
        dwi = subject['dwi']
        flair = subject['flair']
        adc = subject['adc']
        mask = subject['mask']

        min_slices = min(dwi.shape[-1], flair.shape[-1], adc.shape[-1], mask.shape[-1])
        slice_idx = min_slices // 2

        dwi_slice = self.preprocess_image(dwi, slice_idx)
        flair_slice = self.preprocess_image(flair, slice_idx)
        adc_slice = self.preprocess_image(adc, slice_idx)
        mask_slice = (mask[..., slice_idx] > 0).astype(np.uint8)
        mask_img = Image.fromarray(mask_slice * 255).resize(dwi_slice.size)  

        if self.image_transform:
            dwi_slice = self.image_transform(dwi_slice.convert('RGB'))
            flair_slice = self.image_transform(flair_slice.convert('RGB'))
            adc_slice = self.image_transform(adc_slice.convert('RGB'))
        if self.mask_transform:
            mask_img = self.mask_transform(mask_img)

        # Concatenate slices along the channel dimension
        input_tensor = torch.cat([dwi_slice, flair_slice, adc_slice], dim=0)

        return id_, input_tensor, mask_img


class UNet(nn.Module):
    def __init__(self, dropout_prob=0.5):
        super(UNet, self).__init__()
        # Encoder
        self.enc1 = nn.Sequential(
            nn.Conv2d(9, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool1 = nn.MaxPool2d(2, 2)
        self.enc2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool2 = nn.MaxPool2d(2, 2)
        self.enc3 = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool3 = nn.MaxPool2d(2, 2)
        # Bottleneck
        self.bottleneck = nn.Sequential(
            nn.Conv2d(256, 512, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 512, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        # Decoder
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)
        self.dec3 = nn.Sequential(
            nn.Conv2d(512, 256, 3, padding=1),  
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1),  
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)
        self.dec1 = nn.Sequential(
            nn.Conv2d(128, 64, 3, padding=1),  
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.final_conv = nn.Conv2d(64, 1, 1)
        self.dropout = nn.Dropout(dropout_prob)

    def forward(self, x):
        # Encode
        enc1_out = self.enc1(x)
        enc2_in = self.pool1(enc1_out)
        enc2_out = self.enc2(enc2_in)
        enc3_in = self.pool2(enc2_out)
        enc3_out = self.enc3(enc3_in)
        bottleneck_in = self.pool3(enc3_out)
        # Bottleneck
        bottleneck_out = self.bottleneck(bottleneck_in)
        # Decode
        upconv3_out = self.upconv3(bottleneck_out)
        dec3_in = torch.cat((upconv3_out, enc3_out), dim=1)
        dec3_out = self.dec3(dec3_in)
        upconv2_out = self.upconv2(dec3_out)
        dec2_in = torch.cat((upconv2_out, enc2_out), dim=1)
        dec2_out = self.dec2(dec2_in)
        upconv1_out = self.upconv1(dec2_out)
        dec1_in = torch.cat((upconv1_out, enc1_out), dim=1)
        dec1_out = self.dec1(dec1_in)
        final_out = self.final_conv(dec1_out)
        return final_out

class DiceJaccardLoss(nn.Module):
    def __init__(self):
        super(DiceJaccardLoss, self).__init__()

    def forward(self, outputs, targets):
        # Convert outputs to probabilities using sigmoid
        outputs = torch.sigmoid(outputs)
        # Flatten the tensors
        outputs = outputs.view(-1)
        targets = targets.view(-1)
        # Calculate Dice coefficient
        intersection = (outputs * targets).sum()
        dice = (2. * intersection) / (outputs.sum() + targets.sum() + 1e-6)
        print(f"Dice Value: {dice4f}")
        # Calculate Jaccard index
        union = outputs.sum() + targets.sum() - intersection
        jaccard = intersection / (union + 1e-6)
        # Combine Dice and Jaccard losses
        loss = 1 - dice + 1 - jaccard
        return loss


subjects = load_isles_data(data_dir)
train_val_subjects, test_subjects = train_test_split(subjects, test_size=0.2, random_state=42)
train_val_dataset = ISLESSegmentationDataset(train_val_subjects, image_transform=image_transform, mask_transform=mask_transform)
test_dataset = ISLESSegmentationDataset(test_subjects, image_transform=image_transform, mask_transform=mask_transform)

image_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
mask_transform = transforms.Compose([
    transforms.ToTensor(),
])

test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

def reset_weights(m):
    """
    Try resetting model weights to avoid weight leakage.
    """
    for layer in m.children():
        if hasattr(layer, 'reset_parameters'):
            # print(f'Resetting {layer}')
            layer.reset_parameters()

def train_one_epoch(model, train_loader, criterion, optimizer):
    model.train()
    train_loss = 0.0
    for ids, images, masks in train_loader:
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    return train_loss / len(train_loader)

def validate(model, val_loader, criterion):
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for ids, images, masks in val_loader:
            images, masks = images.to(device), masks.to(device)
            outputs = model(images)
            loss = criterion(outputs, masks)
            val_loss += loss.item()
    avg_val_loss = val_loss / len(val_loader)
    return avg_val_loss

def k_fold_cross_validation(model_fn, criterion, optimizer_fn, num_epochs, k_splits=5, dataset=None):
    kf = KFold(n_splits=k_splits, shuffle=True, random_state=42)
    fold_results = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset), 1):
        print(f"Fold {fold}/{k_splits}")
        # Dynamically create model and optimizer
        model = model_fn().to(device)
        model.apply(reset_weights)
        optimizer = optimizer_fn(model.parameters())
        
        train_subset = torch.utils.data.Subset(dataset, train_idx)
        val_subset = torch.utils.data.Subset(dataset, val_idx)
        
        train_loader = DataLoader(train_subset, batch_size=6, shuffle=True)
        val_loader = DataLoader(val_subset, batch_size=6, shuffle=False)
        
        for epoch in range(num_epochs):
            train_loss = train_one_epoch(model, train_loader, criterion, optimizer)
            val_loss = validate(model, val_loader, criterion)
            print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        fold_results.append((train_loss, val_loss))
    return fold_results

best_learning_rate = 0.0001  # Example value, adjust based on your model's needs

model = UNet().to(device)
optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)
criterion = DiceJaccardLoss()

# Model instantiation function
def model_fn():
    return UNet()

# Optimizer function
def optimizer_fn(params):
    return optim.Adam(params, lr=0.0001)

# Assuming train_val_dataset is defined and available
fold_results = k_fold_cross_validation(model_fn, DiceJaccardLoss(), optimizer_fn, num_epochs=10, dataset=train_val_dataset)

def visualize_single_example(image, ground_truth, prediction):
    """Visualize a single example including the image, ground truth mask, and prediction."""
    # Assuming image, ground_truth, and prediction are numpy arrays with shape [batch_size, height, width].
    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    # Image
    axs[0].imshow(image, cmap='gray')
    axs[0].set_title('Image')
    axs[0].axis('off')

    # Ground Truth Mask
    axs[1].imshow(ground_truth, cmap='gray')
    axs[1].set_title('Ground Truth Mask')
    axs[1].axis('off')

    # Predicted Mask
    axs[2].imshow(prediction, cmap='gray')
    axs[2].set_title('Predicted Mask')
    axs[2].axis('off')

    plt.show()

def generate_predictions(model, dataloader, device):
    model.eval()
    all_images, all_masks = [], []
    with torch.no_grad():
        for data in dataloader:
            # Assuming the batch structure is (images, masks)
            images, masks = data  # Unpack the tuple
            images = images.to(device)
            masks = masks.to(device)
            outputs = model(images)
            predictions = torch.sigmoid(outputs)
            predictions = (predictions > 0.5).float()
            # Append the images and masks for this batch
            all_images.append(images.cpu().numpy())
            all_masks.append(masks.cpu().numpy())
    
    # Concatenate the lists along the batch dimension
    all_images = np.concatenate(all_images, axis=0)
    all_masks = np.concatenate(all_masks, axis=0)
    
    return all_images, all_masks
    

# Assuming test_loader is your DataLoader for the test dataset
images, ground_truth_masks, predicted_masks = generate_predictions(model, test_loader, device)

# Assuming you want to visualize the first example from the batch
visualize_single_example(images[0], ground_truth_masks[0], predicted_masks[0])
'''
def visualize_test_predictions(test_loader, model, device):
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations
        for i in range(batch_size):  
        # Extract the correct channel for each modality
        flair_image = images_np[i, :, :, 3]  # First channel for FLAIR
        adc_image = images_np[i, :, :, 6]   # First channel for ADC
        dwi_image = images_np[i, :, :, 0]  # First channel for DWI
        gt_mask = masks_np[i]  # Ground truth mask
        predicted_mask = predictions_np[i]  # Predicted mask
        patient_id = ids[i]  # Get the patient ID for the current index

        # Extract numeric part of the patient ID and remove leading zeros
        numeric_id = int(patient_id.strip('sub-strokecase'))  # Convert the numeric part to int to remove leading zeros

        # Plotting FLAIR with numeric patient ID
        axes[0, i].imshow(flair_image, cmap='gray')
        axes[0, i].set_title(f"FLAIR - {numeric_id}")
        axes[0, i].axis('off')

        # Plotting ADC with numeric patient ID
        axes[1, i].imshow(adc_image, cmap='gray')
        axes[1, i].set_title(f"ADC - {numeric_id}")
        axes[1, i].axis('off')

        # Plotting DWI with numeric patient ID
        axes[2, i].imshow(dwi_image, cmap='gray')
        axes[2, i].set_title(f"DWI - {numeric_id}")
        axes[2, i].axis('off')

        # Plotting ground truth mask with numeric patient ID
        axes[3, i].imshow(dwi_image, cmap='gray')  # Using DWI as background for mask visualization
        axes[3, i].imshow(gt_mask, cmap='hot', alpha=0.6)
        axes[3, i].set_title(f"Mask - {numeric_id}")
        axes[3, i].axis('off')

        # Plotting predicted mask with numeric patient ID
        axes[4, i].imshow(dwi_image, cmap='gray')  # Using DWI as background for prediction visualization
        axes[4, i].imshow(predicted_mask, cmap='hot', alpha=0.6)
        axes[4, i].set_title(f"Prediction - {numeric_id}")
        axes[4, i].axis('off')

    plt.tight_layout()
    plt.show()'''


#visualize_single_example(test_loader, model, device)

# Close the SummaryWriter
writer.close()

torch.save(model.state_dict(), 'path_to_save_model.pth') #Konner this is the part that you will have to edit when you run it make sure the run gives you different val loss.
#there should be no consistent numbers
